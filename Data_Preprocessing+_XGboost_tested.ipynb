{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZnHrwznsei4J"
      },
      "outputs": [],
      "source": [
        "#importing all necessary modules\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "import math\n",
        "import time\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "\n",
        "sns.set_theme()\n",
        "sns.set_palette(\"deep\")\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wVXkezjSro--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b02359-aa8a-41dd-9be3-c297640b188a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[201501.0, '3 ROOM', 60.0, 'IMPROVED', 1986.0, 255000.0], [201501.0, '3 ROOM', 68.0, 'NEW GENERATION', 1981.0, 275000.0], [201501.0, '3 ROOM', 69.0, 'NEW GENERATION', 1980.0, 285000.0], [201501.0, '3 ROOM', 68.0, 'NEW GENERATION', 1979.0, 290000.0], [201501.0, '3 ROOM', 68.0, 'NEW GENERATION', 1980.0, 290000.0], [201501.0, '3 ROOM', 67.0, 'NEW GENERATION', 1980.0, 290000.0], [201501.0, '3 ROOM', 68.0, 'NEW GENERATION', 1980.0, 290000.0], [201501.0, '3 ROOM', 68.0, 'NEW GENERATION', 1981.0, 293000.0], [201501.0, '3 ROOM', 67.0, 'NEW GENERATION', 1978.0, 300000.0], [201501.0, '3 ROOM', 68.0, 'NEW GENERATION', 1985.0, 307500.0]]\n",
            "[201912.0, 'EXECUTIVE', 145.0, 'MAISONETTE', 1987.0, 618000.0]\n"
          ]
        }
      ],
      "source": [
        "#1: selecting relevant features\n",
        "\n",
        "master = [] #Initialize list to store the relevant data in\n",
        "\n",
        "#edited according to which files needed processing\n",
        "files = ['20152016.csv', \"2017onwards.csv\"]\n",
        "filelengths = [] #i added this as an add on to the number indexing\n",
        "for filename in files:\n",
        "  counter = 0\n",
        "  with open(filename, 'r') as f:\n",
        "    header = next(f)\n",
        "    for row in f:\n",
        "      lis = row.strip().split(',')\n",
        "      lis[0] = lis[0][0:4] + lis[0][5:] #processing string with '-' into an integer value\n",
        "      line = [float(lis[0]), lis[2], float(lis[6]), lis[7], float(lis[8]), float(lis[10])] #referencing relevent features and adding them [For dataset of 2017onwards, last val index +1]\n",
        "      line[3] = line[3].upper()\n",
        "      master.append(line)\n",
        "      counter += 1\n",
        "    filelengths.append(counter)\n",
        "\n",
        "#This line to select out the 2020-2024 sales. Comment off if running another year set\n",
        "#master = master[64257:196985]\n",
        "\n",
        "\n",
        "#This line to select out the 2015-2019 sales. Comment off if running another year set\n",
        "master = master[:filelengths[0]+64256]\n",
        "print(master[:10])\n",
        "print(master[-1])\n",
        "\n",
        "#initialize values for better and more intuitive referencing of indexes with minimal confusion\n",
        "month, flat_type, floor_area_sqm, flat_model, lease_commence_date, resale_price, unemployment_rate, real_gdp, population_size = 0,1,2,3,4,5,6,7,8\n",
        "\n",
        "#2: encoding categorical values for flat_model and flat_type\n",
        "enc = OrdinalEncoder()\n",
        "X = [['1 ROOM'], ['2 ROOM'], ['3 ROOM'], ['4 ROOM'], ['5 ROOM'], ['EXECUTIVE'], ['MULTI-GENERATION'], ['MULTI GENERATION']]\n",
        "X_2 = [['SIMPLIFIED'], ['IMPROVED'], ['MODEL A'], ['APARTMENT'], ['NEW GENERATION'], ['STANDARD'], ['MAISONETTE'], ['MODEL A-MAISONETTE'], ['TERRACE'], ['IMPROVED-MAISONETTE'], ['PREMIUM APARTMENT'], ['MULTI GENERATION'], ['2-ROOM'], ['MODEL A2'], ['PREMIUM MAISONETTE'], ['ADJOINED FLAT'], ['TYPE S1'], ['DBSS'], ['PREMIUM APARTMENT LOFT'], ['TYPE S2'], ['3GEN']]\n",
        "\n",
        "enc.fit(X)\n",
        "result = enc.transform([[row[flat_type]] for row in master]) #flat type encoding\n",
        "enc.fit(X_2)\n",
        "result2 = enc.transform([[row[flat_model]] for row in master]) #flat model encoding\n",
        "\n",
        "for row in range(len(master)):\n",
        "  master[row][flat_type] = result[row][0]\n",
        "  master[row][flat_model] = result2[row][0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "csmWggQ7ASR-"
      },
      "outputs": [],
      "source": [
        "#3: adding unemployment rate as a feature to the master dataset\n",
        "import csv\n",
        "#list of sublists seperated by quarters\n",
        "months = [['01', '02', '03'], ['04', '05', '06'], ['07', '08', '09'], ['10', '11', '12']]\n",
        "\n",
        "dic = {} #dic will store manually created string keys ('20241Q'...'19904Q') that will reference sublists consisting of months(from the months list) in that specific quarter.\n",
        "#eg. {'20241Q':['202401.0', '202402.0', '202403.0']}\n",
        "for i in range(1990, 2025):\n",
        "  for j in range(1, 5):\n",
        "    dic[str(i)+str(j)+'Q'] = []\n",
        "    for m in months[j-1]:\n",
        "      dic[str(i)+str(j)+'Q'] += [float(str(i)+m)]\n",
        "\n",
        "finaldic = {}\n",
        "\n",
        "with open('UnemploymentRateEndOfPeriodQuarterlySeasonallyAdjusted.csv', 'r') as f:\n",
        "  masterr = [] #this list will contain 2 rows: row 1 contains the quarterly label year values such as '20241Q'. row 2 contains the unemployment rate for that quarter such as '1.7'\n",
        "  for row in f:\n",
        "    row = row.strip().split(',')\n",
        "    masterr.append(row[1:]) #removing the titles total unemployment rate, resident unemployment rate\n",
        "  masterr = masterr[:2] #removing resident and citize unemployment rate\n",
        "  masterr[1] = masterr[1][1:] #removing the title ' (SA)\"'\n",
        "\n",
        "#as the file only contains unemployment rates from 1992-2024,\n",
        "#through research(https://stats.mom.gov.sg/iMAS_Tables/Times%20Series%20Table/mrsd_14_Historical_Unemployment_Rate_28Jan21.xlsx)\n",
        "#we will be manually inserting unemployment rates as 1.7 for each quarter in 1990 and 1991.\n",
        "for i in range(1, 5):\n",
        "  masterr[0].append('1990'+str(i)+'Q')\n",
        "  masterr[0].append('1991'+str(i)+'Q')\n",
        "  masterr[1] += [1.7, 1.7]\n",
        "\n",
        "#for each quarter label in the first row, and for each key in dic, if they equate to each other,\n",
        "#we will insert into finaldic, (the values in the sublist that the key in dic refers to) as the key\n",
        "#that references the (unemployment rate for that month)value.\n",
        "for item in masterr[0]:\n",
        "  for row in dic:\n",
        "    if item == row:\n",
        "      for i in dic[row]:\n",
        "        finaldic[i] = float(masterr[1][masterr[0].index(item)])\n",
        "\n",
        "#adding the correct unemployment rate to the correct month in the dataset based on the float key of the month in finaldic\n",
        "for row in master:\n",
        "  for item in finaldic:\n",
        "    if row[month] == item:\n",
        "      row += [finaldic[item]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "puUkwPvHAieI"
      },
      "outputs": [],
      "source": [
        "#4: calculating remaining lease by performing the following: 99 - (current year - lease commencement date)\n",
        "for row in master:\n",
        "  result = 99-(int(str(row[month])[:4])-int(str(row[lease_commence_date])[:4]))\n",
        "  row[lease_commence_date] = result\n",
        "\n",
        "remaining_lease = lease_commence_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jc3j5DffIeVZ"
      },
      "outputs": [],
      "source": [
        "#5 inserting real gdp into the dataset\n",
        "with open('M015651.csv', 'r') as f:\n",
        "  finalliz = f.readlines()\n",
        "  year = (finalliz[10].strip().split(','))[1:] #removing irrelevant data and the header value to obtain the years\n",
        "  r = year.index('1990 1Q ') #finding the index to cut off and obtain relevant years\n",
        "  year = year[:r+1]\n",
        "  gdp = (finalliz[11].strip().split(','))[1:] #removing irrelevant data and the header value to obtain the gdp\n",
        "  gdp = gdp[:r+1]\n",
        "\n",
        "for row in range(len(year)):\n",
        "  year[row] = year[row][:4] + year[row][5:7] #removing spaces from the quearterly year labels\n",
        "\n",
        "finalfinaldic = {}\n",
        "\n",
        "#for each quarter label in the year list, and for each key in dic, if they equate to each other,\n",
        "#we will insert into finalfinaldic, (the values in the sublist that the key in dic refers to) as the key\n",
        "#that references the (gdp)value in the gdp list.\n",
        "for item in year:\n",
        "  for row in dic:\n",
        "    if item == row:\n",
        "      for i in dic[row]:\n",
        "        finalfinaldic[i] = float(gdp[year.index(item)])\n",
        "\n",
        "#adding the correct gdp rate to the correct month in the dataset based on the float key of the month in finaldic\n",
        "for row in master:\n",
        "  for item in finalfinaldic:\n",
        "    if row[month] == item:\n",
        "      row += [finalfinaldic[item]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1OmKM1K5HYj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmKOTPtMNCDT",
        "outputId": "b30a61b4-029b-424c-9d83-211529be4e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streets split\n",
            "Coordinates calculated\n",
            "Distance calculated\n",
            "Data attached to current dataset\n",
            "Data attached to main dataset\n"
          ]
        }
      ],
      "source": [
        "#6: inserting the distance to cbd\n",
        "\n",
        "#Initializes working data array\n",
        "data = []\n",
        "\n",
        "for filename in files:\n",
        "  with open(filename,'r') as f:\n",
        "    i = 0\n",
        "    for row in f:\n",
        "      if i != 0:\n",
        "        data.append(row.strip('\\n').split(','))\n",
        "      i += 1\n",
        "\n",
        "# For selecting data 2020-2024, comment off if other years\n",
        "#data = data[64257:196985]\n",
        "# For selecting data 2015-2019, comment off if other years\n",
        "data = data[:filelengths[0]+64256]\n",
        "\n",
        "def get_lat_lon(address):\n",
        "    \"\"\"\n",
        "    Gets the latitude and longitude of the street using Nominatim API\n",
        "    \"\"\"\n",
        "    #Initializes the geocoder and collects the location information\n",
        "    geolocator = Nominatim(user_agent=\"Jupyter_AI_Project_HomeBros\")\n",
        "    location = geolocator.geocode(address, timeout=10)\n",
        "\n",
        "    #Begins up to 5 attempts to find location, increasing delay time each attempt in case of timeout\n",
        "    for attempt in range(5):\n",
        "        delay = 1\n",
        "        try:\n",
        "            location = geolocator.geocode(address, timeout=10)\n",
        "\n",
        "            #Checks if location found. If not, prints address and returns None.\n",
        "            #When not found, address was examined to identify why. (During preprocessing. Resulted in the many if statements in the preprocessing function)\n",
        "            if location:\n",
        "                return location.latitude, location.longitude\n",
        "            else:\n",
        "                print(address)\n",
        "                return None, None\n",
        "\n",
        "        except GeocoderTimedOut:\n",
        "            print(f\"Timeout on attempt {attempt+1} for '{address}'... retrying in {delay}s.\")\n",
        "            time.sleep(delay)\n",
        "            delay += 1\n",
        "\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculates shortest distance to CBD in kilometres, accounting for the curve of the Earth\n",
        "    \"\"\"\n",
        "    R = 6371  # Earth's radius in kilometers\n",
        "    phi1 = math.radians(lat1)\n",
        "    phi2 = math.radians(lat2)\n",
        "    delta_phi = math.radians(lat2 - lat1)\n",
        "    delta_lambda = math.radians(lon2 - lon1)\n",
        "\n",
        "    a = math.sin(delta_phi/2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda/2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return round(distance,3)\n",
        "\n",
        "def preprocessing(data, streets, unique_streets):\n",
        "  \"\"\"\n",
        "  Handles shortforms and missing streets\n",
        "  \"\"\"\n",
        "  for row in data[1:]:\n",
        "    new_strt = row[4]\n",
        "\n",
        "    # Storage to retrieve indexes later\n",
        "    if new_strt not in streets:\n",
        "      streets.append(new_strt)\n",
        "\n",
        "    # Processing\n",
        "    if \" NTH \" in new_strt:\n",
        "      new_strt = new_strt.replace(\" NTH \", \" NORTH \")\n",
        "    if \" NTH\" in new_strt:\n",
        "      new_strt = new_strt.replace(\" NTH\", \" NORTH\")\n",
        "    if \" STH \" in new_strt:\n",
        "      new_strt = new_strt.replace(\" STH \", \" SOUTH \")\n",
        "    if \" ST \" in new_strt:\n",
        "      new_strt = new_strt.replace(\" ST \", \" STREET \")\n",
        "    if \" RD \" in new_strt:\n",
        "      new_strt = new_strt.replace(\" RD \", \" ROAD \")\n",
        "    if \"BT \" in new_strt:\n",
        "      new_strt = new_strt.replace(\"BT \", \"BUKIT \")\n",
        "    if \" BT \" in new_strt:\n",
        "      new_strt = new_strt.replace(\" BT \", \" BUKIT \")\n",
        "    if \" ST\" == new_strt[-3:]:\n",
        "      new_strt = new_strt[:-3] + \" STREET\"\n",
        "    if \" RD\" == new_strt[-3:]:\n",
        "      new_strt = new_strt[:-3] + \" ROAD\"\n",
        "    if new_strt == \"JLN MEMBINA BARAT\":\n",
        "      new_strt = \"Central Green Condo\" #Road no longer exists\n",
        "    if \"JLN \" in new_strt:\n",
        "      new_strt = new_strt.replace(\"JLN \", \"JALAN \")\n",
        "    if \"LOR \" in new_strt:\n",
        "      new_strt = new_strt.replace(\"LOR \", \"LORONG \")\n",
        "    if \" AVE \" in new_strt:\n",
        "      new_strt = new_strt.replace(\" AVE \", \" AVENUE \")\n",
        "    if \" AVE\" == new_strt[-4:]:\n",
        "      new_strt = new_strt[:-4] + \" AVENUE\"\n",
        "    if \" DR \" in new_strt:\n",
        "      new_strt = new_strt.replace(\" DR \", \" DRIVE \")\n",
        "    if \" DR\" == new_strt[-3:]:\n",
        "      new_strt = new_strt[:-3] + \" DRIVE\"\n",
        "    if \"C'WEALTH\" in new_strt:\n",
        "      new_strt = new_strt.replace(\"C'WEALTH\", \"COMMONWEALTH\")\n",
        "    if \"TG \" in new_strt:\n",
        "      new_strt = new_strt.replace(\"TG \", \"TANJONG \")\n",
        "    if new_strt == \"KG BAHRU HILL\":\n",
        "      new_strt = \"SPOONER ROAD\" #Road no longer exists\n",
        "    elif \"KG \" in new_strt:\n",
        "      new_strt = new_strt.replace(\"KG \", \"KAMPONG \")\n",
        "    if \"UPP \" in new_strt:\n",
        "      new_strt = new_strt.replace(\"UPP \", \"UPPER \")\n",
        "    if \"BUANGKOK SOUTH FARMWAY 1\" == new_strt:\n",
        "      new_strt = \"BUANGKOK\" #Road no longer exists\n",
        "\n",
        "    #If street not already in unique_streets, add it\n",
        "    if new_strt not in unique_streets:\n",
        "      unique_streets.append(new_strt)\n",
        "  return\n",
        "\n",
        "\n",
        "#Selects unique streets for processing, and stores streets for returning to initial dataset\n",
        "streets = []\n",
        "unique_streets = []\n",
        "preprocessing(data[1:], streets, unique_streets)\n",
        "print(\"Streets split\")\n",
        "\n",
        "#Converts each item in streets to a list in the format [street_name, latitude, longitude]\n",
        "i = 0\n",
        "for row in unique_streets:\n",
        "  address = row\n",
        "  lat, lon = get_lat_lon(address)\n",
        "  unique_streets[i] = [row, lat, lon]\n",
        "  i += 1\n",
        "print(\"Coordinates calculated\")\n",
        "\n",
        "# Initialized the values for CBD's latitude and longitude\n",
        "CBD = [1.2812, 103.8503]\n",
        "# Calculates distance to CBD of each location\n",
        "for row in unique_streets:\n",
        "  dist = haversine(CBD[0], CBD[1], row[1], row[2])\n",
        "  row.append(dist)\n",
        "print(\"Distance calculated\")\n",
        "\n",
        "# Adjusts main dataset to reflect dist to CBD instead of street name\n",
        "for row in data:\n",
        "  if streets.index(row[4]) > len(unique_streets):\n",
        "    print(\"Error\")\n",
        "    print(row[4]) #Preprocessing checks\n",
        "  else:\n",
        "    record = unique_streets[streets.index(row[4])]\n",
        "    row[4] = record[3]\n",
        "print('Data attached to current dataset')\n",
        "\n",
        "# Attached data to main working dataset\n",
        "for row in range(len(data)):\n",
        "  master[row].append(data[row][4])\n",
        "print(\"Data attached to main dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "0N75Ze-GrYwB"
      },
      "outputs": [],
      "source": [
        "#7 Insert population into dataset\n",
        "with open('M810811.csv', 'r') as f:\n",
        "  finalliz = f.readlines()\n",
        "  year = (finalliz[10].strip().split(','))[1:] #removing irrelevant data and the header value to obtain the years\n",
        "  population = (finalliz[11].strip().split(','))[1:] #removing irrelevant data and the header value to obtain the gdp\n",
        "\n",
        "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
        "finalpop = []\n",
        "finalyear = []\n",
        "for row in year:\n",
        "  for m in months:\n",
        "    finalyear += [(row.strip()+m)]\n",
        "\n",
        "for pop in population:\n",
        "  for i in range(12):\n",
        "    finalpop.append(pop)\n",
        "\n",
        "#adding the correct unemployment rate to the correct month in the dataset based on the float key of the month in finaldic\n",
        "for row in master:\n",
        "  for item in finalyear:\n",
        "    if row[month] == float(item):\n",
        "      row += [float(finalpop[finalyear.index(item)])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ev4EScCbscr4"
      },
      "outputs": [],
      "source": [
        "#write to the storage dataset\n",
        "with open(\"20152019processedvalidation.csv\",\"w\") as f:\n",
        "    for row in master:\n",
        "        for each in row:\n",
        "            if type(each) != str:\n",
        "                each = str(each)\n",
        "            f.write(each)\n",
        "            f.write(\",\")\n",
        "        f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas for data wrangling\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# import numpy for Scientific computations\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# import machine learning libraries\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# import packages for hyperparameters tuning\n",
        "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
        "\n",
        "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
        "        'gamma': hp.uniform ('gamma', 1,9),\n",
        "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
        "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
        "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
        "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
        "        'n_estimators': 180,\n",
        "        'seed': 0\n",
        "    }\n",
        "sets = ['19902014processedtraining.csv', '20172019processed.csv']\n",
        "finalmaster = []\n",
        "for i in range(len(sets)):\n",
        "  with open(sets[i], 'r') as f:\n",
        "    master = [] #list to have relevant features from the resale flat price dataset\n",
        "    for row in f:\n",
        "      if len(row) > 1:\n",
        "        lis = row.strip().split(',')\n",
        "        master.append(lis)\n",
        "  y = []\n",
        "  resale_price = 5\n",
        "  for row in master:\n",
        "    y.append(float(row[resale_price]))\n",
        "  X = []\n",
        "  for row in master:\n",
        "    r = row[:resale_price]+row[resale_price+1:]\n",
        "    for val in range(len(r)):\n",
        "      if r[val] != '':\n",
        "        r[val] = float(r[val])\n",
        "      else:\n",
        "        r.pop(val)\n",
        "    X.append(r)\n",
        "  finalmaster.append([X, y])\n",
        "  print(i)\n",
        "\n",
        "X_1 = np.array(finalmaster[0][0])\n",
        "y_1 = np.array(finalmaster[0][1])\n",
        "X_2 = np.array(finalmaster[1][0])\n",
        "y_2 = np.array(finalmaster[1][1])\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 500, 1000],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "xgbr = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1)\n",
        "grid_search = GridSearchCV(estimator=xgbr, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_1, y_1)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "_SBreBXaUFKT",
        "outputId": "609ebd2a-4b3b-4095-b4f7-72e79a3066d9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '20172019processed.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-b70bb126d24d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mfinalmaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mmaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#list to have relevant features from the resale flat price dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20172019processed.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# === FILE PATHS ===\n",
        "train_file = '19902014processedtraining.csv'\n",
        "val_file = '20152019processedvalidation.csv'\n",
        "test_file = '20202024processedtesting.csv'  # Ensure this exists or load later\n",
        "\n",
        "# === UTILITY FUNCTION: LOAD & CLEAN ===\n",
        "sets = [train_file, val_file, test_file]\n",
        "finalmaster = []\n",
        "for i in range(len(sets)):\n",
        "  with open(sets[i], 'r') as f:\n",
        "    master = [] #list to have relevant features from the resale flat price dataset\n",
        "    for row in f:\n",
        "      if len(row) > 1:\n",
        "        lis = row.strip().split(',')\n",
        "        master.append(lis)\n",
        "  y = []\n",
        "  resale_price = 5\n",
        "  for row in master:\n",
        "    y.append(float(row[resale_price]))\n",
        "  X = []\n",
        "  for row in master:\n",
        "    r = row[:resale_price]+row[resale_price+1:]\n",
        "    for val in range(len(r)):\n",
        "      if r[val] != '':\n",
        "        r[val] = float(r[val])\n",
        "      else:\n",
        "        r.pop(val)\n",
        "    X.append(r)\n",
        "  finalmaster.append([X, y])\n",
        "  print(i)\n",
        "\n",
        "# === LOAD DATA ===\n",
        "X_train_raw = np.array(finalmaster[0][0])\n",
        "y_train = np.array(finalmaster[0][1])\n",
        "X_val_raw = np.array(finalmaster[1][0])\n",
        "y_val = np.array(finalmaster[1][1])\n",
        "X_test_raw = np.array(finalmaster[2][0])\n",
        "y_test = np.array(finalmaster[2][1])\n",
        "\n",
        "\n",
        "# === FEATURE SCALING ===\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train_raw)\n",
        "X_val = scaler.transform(X_val_raw)\n",
        "X_test = scaler.transform(X_test_raw)\n",
        "\n",
        "# === MODEL AND TUNING ===\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 500, 1000],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "xgbr = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1)\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgbr,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# === BEST MODEL ===\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best score (Neg MSE):\", grid_search.best_score_)\n",
        "\n",
        "# === VALIDATION PERFORMANCE ===\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "print(f\"Validation RMSE: {rmse_val:.2f}\")\n",
        "\n",
        "# === TEST PERFORMANCE (Optional) ===\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "print(f\"Test RMSE: {rmse_test:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KPn0-IfgiwY",
        "outputId": "95e2602a-d554-4d8c-ac05-ac28fd4de029"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Best parameters: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 1000}\n",
            "Best score (Neg MSE): -9871146870.331621\n",
            "Validation RMSE: 62538.58\n",
            "Test RMSE: 132298.73\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}